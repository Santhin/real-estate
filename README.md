
---

## ğŸ§ About <a name = "about"></a>

Project was created for "SKNS Warsztaty z Pythona". <br>
Consists crawler for scraping real estate data from gumtree and jupyter notebook with ML.

## ğŸ Getting Started <a name = "getting_started"></a>

To clone repository type:
```
git clone https://github.com/Santhin/real-estate
```
To run crawler locally:
```
pip install -r requirements
python app.py
```

### Project structure
```
.
â”œâ”€â”€ crawler
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ aps_asyncio.py
â”‚Â Â  â”œâ”€â”€ gumtree
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ items.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ middlewares.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ pipelines.py
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ settings.py
â”‚Â Â  â”‚Â Â  â””â”€â”€ spiders
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ gumtree_crawler.py
â”‚Â Â  â”‚Â Â      â”œâ”€â”€ __init__.py
â”‚Â Â  â”‚Â Â      â””â”€â”€ stack.py
â”‚Â Â  â”œâ”€â”€ install_asyncio.py
â”‚Â Â  â”œâ”€â”€ Procfile
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ scrapy.cfg
â”œâ”€â”€ LICENSE
â”œâ”€â”€ ml
â”‚Â Â  â”œâ”€â”€ features
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ rankingcen.xlsx
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Ranking\ Dzielnic\ 2020\ Warszawa.pdf
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ranking_dzielnic_warszawy_pod_wzgledem_atrakcyjnosci_warunkow_zycia_2017.pdf
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ranking_otodom.csv
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ranking.txt
â”‚Â Â  â”‚Â Â  â””â”€â”€ ranking.xlsx
â”‚Â Â  â”œâ”€â”€ notebooks
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ML\ endgame\ floydhub.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ ML\ endgame.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ NLP\ eda\ etc.ipynb
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Pipeline\ mongoRaw\ to\ clean\ before\ EDA.ipynb
â”‚Â Â  â”‚Â Â  â””â”€â”€ real\ EDA.ipynb
â”‚Â Â  â””â”€â”€ pictures
â”‚Â Â      â”œâ”€â”€ images.png
â”‚Â Â      â”œâ”€â”€ ml_map.png
â”‚Â Â      â”œâ”€â”€ simple-house-exterior-white-background_1308-50195.jpg
â”‚Â Â      â”œâ”€â”€ unnamed.jpg
â”‚Â Â      â””â”€â”€ white-house-background-check-democratic-party-republican-party-house-png.jpg
â””â”€â”€ README.md

6 directories, 32 files
```


## ğŸš€ Deployment <a name = "deployment"></a>
The crawler was deployed on Heroku and in 15min intervals was activated with advanced python scheduler.

## â›ï¸ Built Using <a name = "built_using"></a>

- [Scrapy](https://scrapy.org/) - Crawler
- [MongoDB](https://www.mongodb.com/) - Database
- [Heroku](https://www.heroku.com/) - Deployment
- [Floydhub](https://www.floydhub.com/) - Traning model


## ğŸ› ï¸ Todo
- add requirements.txt to ML folder