{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.6/site-packages (20.3.3)\n",
      "Requirement already satisfied: pickle5 in /usr/local/lib/python3.6/site-packages (0.0.11)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas) (2018.5)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/site-packages (from pandas) (2.7.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/site-packages (1.19.5)\n",
      "Requirement already satisfied: scikit-learn==0.24.0 in /usr/local/lib/python3.6/site-packages (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/site-packages (from scikit-learn==0.24.0) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/site-packages (from scikit-learn==0.24.0) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/site-packages (from scikit-learn==0.24.0) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/site-packages (from scikit-learn==0.24.0) (1.1.0)\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "floyd-cli 0.11.17 requires click<7,>=6.7, but you have click 7.1.2 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install pickle5\n",
    "!pip install --upgrade pandas\n",
    "!pip install numpy\n",
    "!pip install scikit-learn==0.24.0\n",
    "#!pip install pandas==0.24.1\n",
    "!pip install wandb -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pandas\n",
    "import pandas as pd \n",
    "import pickle5 as pickle\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import numpy as np\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming outliers imputations\n",
    "def median_imputation(df, field_name):\n",
    "    q1 = df[field_name].quantile(0.25)\n",
    "    q1 = df[field_name].quantile(0.25)\n",
    "    q3 = df[field_name].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    Lower_tail = q1 - 1.5 * iqr\n",
    "    Upper_tail = q3 + 1.5 * iqr\n",
    "    med = np.median(df[field_name])\n",
    "    for i in df[field_name]:\n",
    "        if i > Upper_tail or i < Lower_tail:\n",
    "            df[field_name] = df[field_name].replace(i, med)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_pickle(r'data_ML2021_01_07_15_36_52.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['opis', 'cena', 'data dodania', 'lokalizacja', 'Na_sprzedaż_przez',\n",
       "       'Rodzaj_nieruchomosci', 'Liczba_pokoi', 'Liczba_łazienek',\n",
       "       'Wielkość (m2)', 'Parking', 'mieszkanie_url', 'dzielnica', 'miasto',\n",
       "       'cena_za_metr', 'atrakcyjnosc', 'poziom_atrakcyjnosci',\n",
       "       'ekologia_ranking', 'bezpieczenstwo_ranking', 'opis_clean',\n",
       "       'opis_cleanv2', 'opis_clean_nlp', 'opis_cleanv2_nlp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.4364543261896323, total=   0.1s\n",
      "[CV] ....................... , score=0.4467320347677121, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1757s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.4338428205592179, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.4433085604955055, total=   0.2s\n",
      "[CV] ...................... , score=0.45537371045307357, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    0.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4364543261896323, 0.4467320347677121, 0.4433085604955055, 0.4338428205592179, 0.45537371045307357]\n",
      "\n",
      "Accuracy: 0.44 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "### ML without opis with outliers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','opis_cleanv2_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, cv=5,n_jobs=-1,verbose=15)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=3555631.7581710448, total=   0.1s\n",
      "[CV] ....................... , score=3594786.8792736945, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Batch computation too fast (0.1770s.) Setting batch_size=2.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=3790972.2953676092, total=   0.2s\n",
      "[CV] ....................... , score=3648859.6293048416, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] ........................ , score=3516623.601419095, total=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed:    0.4s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3555631.7581710448, 3594786.8792736945, 3648859.6293048416, 3790972.2953676092, 3516623.601419095]\n",
      "\n",
      "Mean square error: 3621374.83 (+/- 190831.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "scoring='f1_macro'\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','opis_cleanv2_nlp','data dodania'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "scores = cross_val_score(LinearRegression(), X_train, y_train, scoring=make_scorer(mean_squared_error), cv=5, n_jobs=-1,verbose=15)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Mean square error: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=0.40087248102319206, total=  15.1s\n",
      "[CV] ...................... , score=0.39726006470809405, total=  15.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   15.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.3987065341340176, total=   8.2s\n",
      "[0.39726006470809405, 0.40087248102319206, 0.3987065341340176]\n",
      "\n",
      "Accuracy: 0.40 (+/- 0.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   23.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   23.9s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis_cleanv2_nlp')),\n",
    "                ('tfidf', TfidfVectorizer(smooth_idf=True)),\n",
    "                ('best', TruncatedSVD(n_components=250)),\n",
    "                ('linear', LinearRegression())\n",
    "            ])\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=3,n_jobs=-1,verbose=15)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.3626722577411884, total= 7.8min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  7.8min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.3622469109800719, total= 8.3min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:  8.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ , score=0.378221852992373, total= 7.9min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed: 15.7min remaining: 10.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.3594149666522408, total= 8.4min\n",
      "[CV] ....................... , score=0.3370261830150276, total= 6.6min\n",
      "[0.3626722577411884, 0.3622469109800719, 0.378221852992373, 0.3594149666522408, 0.3370261830150276]\n",
      "\n",
      "Accuracy: 0.36 (+/- 0.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 22.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 22.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis_cleanv2_nlp')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=250)),\n",
    "                ('linear', SVR(kernel='linear', C=1000, gamma='auto'))\n",
    "            ])\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5,verbose=15,n_jobs=-1)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/externals/joblib/numpy_pickle.py:93: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  pickler.file_handle.write(chunk.tostring('C'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] ....................... , score=0.3823134329707503, total=10.9min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 10.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.3828936858474194, total=11.5min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 11.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ........................ , score=0.405599255960666, total=11.0min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   5 | elapsed: 21.9min remaining: 14.6min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....................... , score=0.3915199671608081, total=10.9min\n",
      "[CV] ....................... , score=0.3666822775615214, total= 9.4min\n",
      "[0.3828936858474194, 0.3823134329707503, 0.405599255960666, 0.3915199671608081, 0.3666822775615214]\n",
      "\n",
      "Accuracy: 0.39 (+/- 0.03)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 31.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 31.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#sklearn.svm.LinearSVR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis_cleanv2_nlp')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=350)),\n",
    "                ('linear', SVR(kernel='linear', C=1000, gamma='auto'))\n",
    "            ])\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5,n_jobs=-1,verbose=15)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis_cleanv2_nlp')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=350)),\n",
    "                ('linear', LinearSVR(C=1000))\n",
    "            ])\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "scores = cross_val_score(pipeline, X_train, y_train, cv=5,n_jobs=-1,verbose=15)\n",
    "print(list(scores))\n",
    "print()\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "parameters = parameters = {\n",
    "    'best__n_components': (350,500,750,1000),\n",
    "    'linear__C': (100,1000,10000)\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis_cleanv2_nlp')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD()),\n",
    "                ('linear', LinearSVR())\n",
    "            ])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=3,n_jobs=-1,verbose=15)\n",
    "\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] best__n_components=750, svr__C=100, svr__kernel=linear ..........\n",
      "[CV] best__n_components=750, svr__C=100, svr__kernel=linear ..........\n",
      "[CV]  best__n_components=750, svr__C=100, svr__kernel=linear, score=0.3802497743777943, total=16.6min\n",
      "[CV] best__n_components=750, svr__C=100, svr__kernel=linear ..........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed: 26.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  best__n_components=750, svr__C=100, svr__kernel=linear, score=0.3808465125011462, total=16.7min\n",
      "[CV] best__n_components=750, svr__C=100, svr__kernel=rbf .............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed: 27.1min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "parameters = parameters = {\n",
    "    'best__n_components': (750,1000),\n",
    "    'svr__C': (100,1000),\n",
    "    'svr__kernel':('linear', 'rbf')\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                ('selector', ItemSelector(key='opis_cleanv2_nlp')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD()),\n",
    "                ('svr', SVR())\n",
    "            ])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=30, cv=3,n_jobs=-1)\n",
    "\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scale': ['passthrough', StandardScaler(copy=True, with_mean=True, with_std=True), Normalizer(copy=True, norm='l2')]}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform. 'passthrough' (type <class 'str'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-98e5cda0d984>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m pipe = Pipeline([\n\u001b[1;32m     27\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m'scale'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m'regression'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRidge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m ])\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, steps, memory)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 raise TypeError(\"All intermediate steps should be \"\n\u001b[1;32m    161\u001b[0m                                 \u001b[0;34m\"transformers and implement fit and transform.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m                                 \" '%s' (type %s) doesn't\" % (t, type(t)))\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# We allow last estimator to be None as an identity transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform. 'passthrough' (type <class 'str'>) doesn't"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore', category=DeprecationWarning)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = dict( scale=['passthrough', StandardScaler(), Normalizer()]\n",
    ")\n",
    "                  \n",
    "print(param_grid)\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','opis_clean_nlp','data dodania','opis_cleanv2_nlp'], axis=1)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scale',  'passthrough'),\n",
    "    ('regression', Ridge())\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, verbose=30, cv=3,n_jobs=-1)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=17)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search.best_estimator_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 5184 candidates, totalling 10368 fits\n",
      "[CV 1/2; 1/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 1/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 2/2; 1/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 1/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 2/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 2/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.8s\n",
      "[CV 2/2; 2/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 2/2; 2/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 3/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 3/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   1.8s\n",
      "[CV 2/2; 3/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 3/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 4/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 4/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 4/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 4/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 5/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 5/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 5/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 2/2; 5/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 6/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 6/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 6/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 6/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 7/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 7/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 7/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 7/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 8/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 8/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 8/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 2/2; 8/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 9/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 9/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 9/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 9/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 10/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 10/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 10/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 10/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 11/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 11/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 11/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 2/2; 11/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 12/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 12/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 12/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 12/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 13/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 13/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 13/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 13/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 14/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 14/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 14/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 2/2; 14/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 15/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 15/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 15/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 15/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=passthrough, union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   2.1s\n",
      "[CV 1/2; 16/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 16/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   1.6s\n",
      "[CV 2/2; 16/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 16/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 17/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 17/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.7s\n",
      "[CV 2/2; 17/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 2/2; 17/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 18/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 18/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   1.6s\n",
      "[CV 2/2; 18/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 18/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=StandardScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 19/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 1/2; 19/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   1.6s\n",
      "[CV 2/2; 19/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}\n",
      "[CV 2/2; 19/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 3.0, 'description': 1.0}; total time=   2.0s\n",
      "[CV 1/2; 20/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n",
      "[CV 1/2; 20/5184] END regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}; total time=   1.6s\n",
      "[CV 2/2; 20/5184] START regressor=SVR(C=10000), union__description__best__n_components=650, union__description__scaler2=passthrough, union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=Normalizer(), union__transformer_weights={'table': 2.0, 'description': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys=[]):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop(self.keys, axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('union', \n",
    "        FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('table', \n",
    "                    Pipeline([\n",
    "                        ('selector1', ItemUnSelector(keys=['opis_cleanv2_nlp', 'opis_clean_nlp'])),\n",
    "                        ('scaler1', 'passthrough')\n",
    "                    ])\n",
    "                ),\n",
    "                ('description', \n",
    "                    Pipeline([\n",
    "                        ('selector2', ItemSelector()),\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('best', TruncatedSVD()),\n",
    "                        ('scaler2', 'passthrough')\n",
    "                    ])\n",
    "                )\n",
    "            ]\n",
    "        )    \n",
    "\n",
    "   ),\n",
    "   ('regressor', \n",
    "        TransformedTargetRegressor()\n",
    "    )\n",
    "])\n",
    "\n",
    "parameters = parameters = {\n",
    "    'union__transformer_weights': [ { 'table': 3.0, 'description': 1.0}, { 'table': 2.0, 'description': 1.0}, { 'table': 1.0, 'description': 1.0}],\n",
    "\n",
    "    'union__description__best__n_components': (650, 700, 750),\n",
    "    'union__description__tfidf__min_df': (3, 4, 5),\n",
    "    'union__description__tfidf__binary': (True,False),\n",
    "    'union__description__selector2__key': ['opis_cleanv2_nlp', 'opis_clean_nlp'] ,\n",
    "\n",
    "    'union__table__scaler1': ['passthrough', StandardScaler(), Normalizer(), RobustScaler()],\n",
    "    'union__description__scaler2': ['passthrough', StandardScaler(), Normalizer(), RobustScaler(with_centering=False)],\n",
    "\n",
    "    'regressor': [SVR(kernel='rbf', C=10000), SVR(kernel='linear', C=10000), GradientBoostingRegressor()] ,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=30, cv=2)\n",
    "\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data[:2000],columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','data dodania'], axis=1)\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search.cv_results_)\n",
    "print(grid_search.best_score_)\n",
    "print()\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5; 1/1] START regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/5; 1/1] END regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=  24.6s\n",
      "[CV 2/5; 1/1] START regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/5; 1/1] END regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=  19.9s\n",
      "[CV 3/5; 1/1] START regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 3/5; 1/1] END regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=  21.4s\n",
      "[CV 4/5; 1/1] START regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 4/5; 1/1] END regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=  23.9s\n",
      "[CV 5/5; 1/1] START regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 5/5; 1/1] END regressor=GradientBoostingRegressor(), union__description__best__n_components=700, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time=  23.3s\n",
      "done in 141.278s\n",
      "Best score: 0.6167154650477379\n",
      "Best parameters set:\n",
      "\n",
      "\tregressor: GradientBoostingRegressor()\n",
      "\tunion__description__best__n_components: 700\n",
      "\tunion__description__scaler2: RobustScaler(with_centering=False)\n",
      "\tunion__description__selector2__key: 'opis_cleanv2_nlp'\n",
      "\tunion__description__tfidf__binary: True\n",
      "\tunion__description__tfidf__min_df: 3\n",
      "\tunion__table__scaler1: RobustScaler()\n",
      "\tunion__transformer_weights: {'table': 1.0, 'description': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys=[]):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop(self.keys, axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('union', \n",
    "        FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('table', \n",
    "                    Pipeline([\n",
    "                        ('selector1', ItemUnSelector(keys=['opis_cleanv2_nlp', 'opis_clean_nlp'])),\n",
    "                        ('scaler1', 'passthrough')\n",
    "                    ])\n",
    "                ),\n",
    "                ('description', \n",
    "                    Pipeline([\n",
    "                        ('selector2', ItemSelector()),\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('best', TruncatedSVD()),\n",
    "                        ('scaler2', 'passthrough')\n",
    "                    ])\n",
    "                )\n",
    "            ]\n",
    "        )    \n",
    "\n",
    "   ),\n",
    "   ('regressor', \n",
    "        TransformedTargetRegressor()\n",
    "    )\n",
    "])\n",
    "\n",
    "parameters = parameters = {\n",
    "    'union__transformer_weights': [  { 'table': 1.0, 'description': 1.0}],\n",
    "\n",
    "    'union__description__best__n_components': (700,),\n",
    "    'union__description__tfidf__min_df': (3,),\n",
    "    'union__description__tfidf__binary': (True,),\n",
    "    'union__description__selector2__key': [ 'opis_cleanv2_nlp'] ,\n",
    "    \n",
    "    'union__table__scaler1': [ RobustScaler()],\n",
    "    'union__description__scaler2': [ RobustScaler(with_centering=False)],\n",
    "    \n",
    "    'regressor': [ GradientBoostingRegressor()] ,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=20, cv=5)\n",
    "\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data[:2000],columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','data dodania'], axis=1)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print()\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV 1/2; 1/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 1/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.2min\n",
      "[CV 2/2; 1/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 1/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.4min\n",
      "[CV 1/2; 2/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 2/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.2min\n",
      "[CV 2/2; 2/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 2/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=True, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.4min\n",
      "[CV 1/2; 3/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 3/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.3min\n",
      "[CV 2/2; 3/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 3/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=3, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.4min\n",
      "[CV 1/2; 4/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 1/2; 4/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.2min\n",
      "[CV 2/2; 4/4] START regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}\n",
      "[CV 2/2; 4/4] END regressor=GradientBoostingRegressor(), union__description__best__n_components=650, union__description__scaler2=RobustScaler(with_centering=False), union__description__selector2__key=opis_cleanv2_nlp, union__description__tfidf__binary=False, union__description__tfidf__min_df=4, union__table__scaler1=RobustScaler(), union__transformer_weights={'table': 1.0, 'description': 1.0}; total time= 5.4min\n",
      "done in 3121.027s\n",
      "Best score: 0.608527733721302\n",
      "Best parameters set:\n",
      "\n",
      "\tregressor: GradientBoostingRegressor()\n",
      "\tunion__description__best__n_components: 650\n",
      "\tunion__description__scaler2: RobustScaler(with_centering=False)\n",
      "\tunion__description__selector2__key: 'opis_cleanv2_nlp'\n",
      "\tunion__description__tfidf__binary: False\n",
      "\tunion__description__tfidf__min_df: 3\n",
      "\tunion__table__scaler1: RobustScaler()\n",
      "\tunion__transformer_weights: {'table': 1.0, 'description': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key=''):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "class ItemUnSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, keys=[]):\n",
    "        self.keys = keys\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict.drop(self.keys, axis=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "   ('union', \n",
    "        FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('table', \n",
    "                    Pipeline([\n",
    "                        ('selector1', ItemUnSelector(keys=['opis_cleanv2_nlp', 'opis_clean_nlp'])),\n",
    "                        ('scaler1', 'passthrough')\n",
    "                    ])\n",
    "                ),\n",
    "                ('description', \n",
    "                    Pipeline([\n",
    "                        ('selector2', ItemSelector()),\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('best', TruncatedSVD()),\n",
    "                        ('scaler2', 'passthrough')\n",
    "                    ])\n",
    "                )\n",
    "            ]\n",
    "        )    \n",
    "\n",
    "   ),\n",
    "   ('regressor', \n",
    "        TransformedTargetRegressor()\n",
    "    )\n",
    "])\n",
    "\n",
    "parameters = parameters = {\n",
    "    'union__transformer_weights': [ { 'table': 1.0, 'description': 1.0}],\n",
    "\n",
    "    'union__description__best__n_components': (650,),\n",
    "    'union__description__tfidf__min_df': (3, 4),\n",
    "    'union__description__tfidf__binary': (True,False),\n",
    "    'union__description__selector2__key': [ 'opis_cleanv2_nlp'] ,\n",
    "    \n",
    "    'union__table__scaler1': [ RobustScaler()],\n",
    "    'union__description__scaler2': [ RobustScaler(with_centering=False)],\n",
    "    \n",
    "    'regressor': [ GradientBoostingRegressor()] ,\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, parameters, verbose=20, cv=2)\n",
    "\n",
    "\n",
    "with open(\"data_ML2021_01_07_15_36_52.pkl\", \"rb\") as fh:\n",
    "  data = pickle.load(fh)\n",
    "median_imputation(data,'cena_za_metr')\n",
    "median_imputation(data,'Wielkość (m2)')\n",
    "testdata = pd.get_dummies(data,columns=['Na_sprzedaż_przez', 'Rodzaj_nieruchomosci','Liczba_pokoi', 'Liczba_łazienek', 'Parking', 'dzielnica','miasto','poziom_atrakcyjnosci'])\n",
    "y = testdata['cena_za_metr'].values\n",
    "X = testdata.drop(['cena_za_metr','cena','mieszkanie_url', 'lokalizacja','opis','opis_clean','opis_cleanv2','data dodania'], axis=1)\n",
    "\n",
    "\n",
    "t0 = time()\n",
    "grid_search.fit(X, y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n",
    "print(f'Best score: {grid_search.best_score_}')\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print()\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.608527733721302\n"
     ]
    }
   ],
   "source": [
    "print(f'Best score: {grid_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
